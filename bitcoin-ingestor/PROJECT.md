# ğŸ§­ AI Development Guide â€” Market Data Ingestion (S3-Only MVP)

## ğŸ¯ Project Overview

This project implements an **MVP (minimum viable product)** for a **real-time Binance market-data ingestion system**.
The system connects to Binance WebSocket streams (e.g., `@trade`) and **stores raw data directly to AWS S3**.

No Kinesis, Lambda, or database components are included in this phase.
The goal is a minimal, reliable pipeline that **writes every event to S3 safely and continuously.**

**Core Objectives**

* Subscribe to Binance `@trade` stream in real time
* Rotate local files every **2 MB or 5 seconds**, whichever comes first
* Compress data (`.jsonl.gz`) and upload to S3
* Create a **manifest JSON** per file (record count, time range, hash, etc.)
* Allow multiple EC2 micro instances (e.g., 3) to run in parallel with duplicate-tolerant design

---

## ğŸ—ï¸ Repository Structure

```
market-data-ingestion/
â”œâ”€â”€ README.md
â”œâ”€â”€ AI_DEV_GUIDE.md               # â† this file
â”œâ”€â”€ .env.example
â”œâ”€â”€ pyproject.toml or requirements.txt
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ collector.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ collector/
â”‚       â”œâ”€â”€ collector_simple.py
â”‚       â”œâ”€â”€ rotator.py
â”‚       â”œâ”€â”€ s3_uploader.py
â”‚       â”œâ”€â”€ manifest.py
â”‚       â”œâ”€â”€ ws_client.py
â”‚       â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ systemd/
â”‚   â””â”€â”€ market-collector.service
â”‚
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ providers.tf
â”‚       â”œâ”€â”€ s3.tf
â”‚       â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ trade.json
â”‚   â”œâ”€â”€ manifest.json
â”‚   â””â”€â”€ ohlcv1s.parquet.schema
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ S3_LAYOUT.md
â”‚   â””â”€â”€ RUNBOOK.md
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â””â”€â”€ integration/
```

---

## âš™ï¸ Technical Specifications

### 1ï¸âƒ£ S3 Folder & File Layout

```
s3://<RAW_BUCKET>/raw/
  exchange=binance/
    stream=trade/
      symbol=BTCUSDT/
        dt=YYYY-MM-DD/HH/MM/
          part-<instance>-<epoch_ms>.jsonl.gz
          manifest-<instance>-<epoch_ms>.json
```

* **Compression:** gzip (`.jsonl.gz`)
* **Rotation Rule:** rotate when file size â‰¥ 2 MB **or** elapsed â‰¥ 5 seconds
* **Partitioning:** UTC-based time partitions
* **Manifest:** JSON file describing each uploaded chunk

  * includes record count, time_min/max, id_first/last, sha256, s3_key, etc.

---

### 2ï¸âƒ£ `collector_simple.py`

* Connects to Binance WebSocket (`wss://stream.binance.com:9443/stream?streams=<symbol>@trade`)
* Asynchronous (asyncio) main loop
* Appends messages to local `.jsonl` file
* On rotation trigger:

  * Gzip-compress local file
  * Upload to S3 (`ServerSideEncryption=AES256`)
  * Generate and upload manifest JSON
  * Delete local temp files
* Reconnects automatically on error with 1-second backoff
* Required environment variables:

  * `RAW_BUCKET`, `SYMBOL`, `LOCAL_DIR`, `ROT_BYTES`, `ROT_SECS`, `INSTANCE_ID`

---

### 3ï¸âƒ£ Manifest JSON Structure

```json
{
  "version": "1",
  "source": {
    "exchange": "binance",
    "stream": "trade",
    "symbol": "BTCUSDT",
    "instance_id": "i-0abc12345"
  },
  "payload": {
    "s3_key": "raw/.../part-i-0abc12345-1739141034567.jsonl.gz",
    "record_count": 12873,
    "bytes_uncompressed": 3689452,
    "bytes_gzip": 2048123,
    "time_min_ms": 1739141030123,
    "time_max_ms": 1739141034560,
    "id_first": 1234567890,
    "id_last": 1234580762,
    "sha256": "..."
  },
  "created_at_ms": 1739141034570
}
```

---

### 4ï¸âƒ£ `rotator.py`

Defines a `RotatingWriter` class that:

* Writes events to file (`write(event)`)
* Checks rotation condition (`size â‰¥ ROT_BYTES` **or** `elapsed â‰¥ ROT_SECS`)
* On rotation: compress â†’ upload to S3 â†’ generate manifest â†’ reset state

---

### 5ï¸âƒ£ `s3_uploader.py`

Handles S3 uploads using boto3:

* `upload_file()` with `ServerSideEncryption="AES256"`
* Retries up to 3 times with exponential backoff
* Graceful exception logging

---

### 6ï¸âƒ£ `systemd/market-collector.service`

```ini
[Service]
User=ec2-user
Restart=always
RestartSec=5
Environment=RAW_BUCKET=my-raw-bucket
Environment=SYMBOL=BTCUSDT
ExecStart=/usr/bin/python3 /home/ec2-user/collector_simple.py
```

* Automatic restart on crash
* Logs accessible via `journalctl -u market-collector -f`

---

### 7ï¸âƒ£ Terraform (infra/terraform)

Creates an S3 bucket with:

* Versioning enabled
* Server-side encryption (AES256)
* Lifecycle policy (optional: archive to Glacier)

Outputs: `raw_bucket_name`, `bucket_arn`

---

### 8ï¸âƒ£ Testing

* **Unit tests:** rotation logic, manifest creation
* **Integration tests:** mocked S3 upload (use `moto`)
* Run via `pytest`

---

## ğŸ§© Development Rules

| Topic          | Guideline                                          |
| -------------- | -------------------------------------------------- |
| Language       | Python 3.11, asyncio, boto3                        |
| Code style     | PEP8, type hints, logging                          |
| Testing        | pytest + moto for mock S3                          |
| Error handling | retries, reconnect, no silent fails                |
| Performance    | assume â‰¤ 3 MB/s per symbol                         |
| Security       | AES256 encryption, no plaintext secrets            |
| Duplicates     | allowed across instances; dedup handled downstream |
| Infra          | EC2 t3.micro Ã— 3, IAM least privilege              |
| Logging        | stdout â†’ journalctl â†’ (optional) CloudWatch        |

---

## âœ… Definition of Done

* [ ] `collector_simple.py` successfully uploads `.jsonl.gz` and `.json` manifest to S3
* [ ] File rotation occurs exactly every 2 MB or 5 s
* [ ] S3 bucket shows proper partition layout
* [ ] Systemd restart resumes upload without loss
* [ ] Quickstart in README can run end-to-end within 10 minutes

---

## ğŸ§± Future Extensions (post-MVP)

* Add Kinesis + Lambda for 1-second OHLCV aggregation
* Gap recovery via REST API (`/api/v3/aggTrades?fromId=`)
* Athena / Glue integration
* Multi-symbol parallel ingestion
* Docker / ECS deployment

---

## ğŸ”’ Summary

> Connect to Binance â†’ collect JSON events â†’ rotate (2 MB or 5 s) â†’ gzip â†’ upload to S3 + manifest.
> System runs redundantly across 3 micro instances; duplicates are acceptable, loss is not.
> Keep the implementation minimal, robust, and within this specification â€” **no external databases or pipelines**.

---

âœ… **File name:** `AI_DEV_GUIDE.md`
Include this file in the repository root so any AI or human developer can follow the exact requirements when generating code.
